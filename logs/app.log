2024-11-04 08:31:09,740 ERROR src.embeddings Error creating embeddings: You have to specify either decoder_input_ids or decoder_inputs_embeds
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 129, in create_embeddings
    embedding = embedding_model.get_embedding(preprocessed_text)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 107, in get_embedding
    outputs = self.model(**inputs)
              ^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 1664, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 990, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds
2024-11-04 08:31:09,743 ERROR src.embeddings Error building vector store: You have to specify either decoder_input_ids or decoder_inputs_embeds
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 149, in build_vector_store
    embeddings = create_embeddings(texts, embedding_model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 129, in create_embeddings
    embedding = embedding_model.get_embedding(preprocessed_text)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 107, in get_embedding
    outputs = self.model(**inputs)
              ^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 1664, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 990, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds
2024-11-04 08:31:09,744 ERROR __main__ Error processing PDF: You have to specify either decoder_input_ids or decoder_inputs_embeds
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/app.py", line 145, in main
    vector_store = build_vector_store(enhanced_sections, embedding_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 149, in build_vector_store
    embeddings = create_embeddings(texts, embedding_model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 129, in create_embeddings
    embedding = embedding_model.get_embedding(preprocessed_text)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 107, in get_embedding
    outputs = self.model(**inputs)
              ^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 1664, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 990, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds
2024-11-04 08:33:40,665 INFO openai error_code=context_length_exceeded error_message="This model's maximum context length is 8192 tokens. However, your messages resulted in 9467 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
2024-11-04 08:33:40,666 ERROR src.rag_model Error generating answer: This model's maximum context length is 8192 tokens. However, your messages resulted in 9467 tokens. Please reduce the length of the messages.
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/src/rag_model.py", line 118, in get_answer
    completion = openai.ChatCompletion.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9467 tokens. Please reduce the length of the messages.
2024-11-04 08:33:40,674 ERROR __main__ Error generating answer: This model's maximum context length is 8192 tokens. However, your messages resulted in 9467 tokens. Please reduce the length of the messages.
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/app.py", line 157, in main
    answer = get_answer(query, vector_store)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/rag_model.py", line 118, in get_answer
    completion = openai.ChatCompletion.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9467 tokens. Please reduce the length of the messages.
2024-11-04 08:33:55,532 ERROR src.embeddings Error creating embeddings: You have to specify either decoder_input_ids or decoder_inputs_embeds
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 129, in create_embeddings
    embedding = embedding_model.get_embedding(preprocessed_text)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 107, in get_embedding
    outputs = self.model(**inputs)
              ^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 1664, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 990, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds
2024-11-04 08:33:55,535 ERROR src.embeddings Error building vector store: You have to specify either decoder_input_ids or decoder_inputs_embeds
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 149, in build_vector_store
    embeddings = create_embeddings(texts, embedding_model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 129, in create_embeddings
    embedding = embedding_model.get_embedding(preprocessed_text)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 107, in get_embedding
    outputs = self.model(**inputs)
              ^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 1664, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 990, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds
2024-11-04 08:33:55,536 ERROR __main__ Error processing PDF: You have to specify either decoder_input_ids or decoder_inputs_embeds
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/app.py", line 145, in main
    vector_store = build_vector_store(enhanced_sections, embedding_model)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 149, in build_vector_store
    embeddings = create_embeddings(texts, embedding_model)
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 129, in create_embeddings
    embedding = embedding_model.get_embedding(preprocessed_text)
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/embeddings.py", line 107, in get_embedding
    outputs = self.model(**inputs)
              ^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 1664, in forward
    decoder_outputs = self.decoder(
                      ^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/transformers/models/t5/modeling_t5.py", line 990, in forward
    raise ValueError(f"You have to specify either {err_msg_prefix}input_ids or {err_msg_prefix}inputs_embeds")
ValueError: You have to specify either decoder_input_ids or decoder_inputs_embeds
2024-11-04 08:37:39,831 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-04 08:37:39,832 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-04 08:37:55,621 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-04 08:37:55,622 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-04 08:37:58,581 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-04 08:37:58,581 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-04 08:38:01,342 INFO openai error_code=context_length_exceeded error_message="This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
2024-11-04 08:38:01,343 ERROR src.rag_model Error generating answer: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/src/rag_model.py", line 118, in get_answer
    completion = openai.ChatCompletion.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
2024-11-04 08:38:01,355 ERROR __main__ Error generating answer: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/app.py", line 157, in main
    answer = get_answer(query, vector_store)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/rag_model.py", line 118, in get_answer
    completion = openai.ChatCompletion.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
2024-11-04 08:39:44,743 INFO openai error_code=context_length_exceeded error_message="This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
2024-11-04 08:39:44,745 ERROR src.rag_model Error generating answer: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/src/rag_model.py", line 118, in get_answer
    completion = openai.ChatCompletion.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
2024-11-04 08:39:44,746 ERROR __main__ Error generating answer: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/app.py", line 157, in main
    answer = get_answer(query, vector_store)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/rag_model.py", line 118, in get_answer
    completion = openai.ChatCompletion.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
2024-11-04 08:39:54,586 INFO openai error_code=context_length_exceeded error_message="This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages." error_param=messages error_type=invalid_request_error message='OpenAI API error received' stream_error=False
2024-11-04 08:39:54,587 ERROR src.rag_model Error generating answer: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/src/rag_model.py", line 118, in get_answer
    completion = openai.ChatCompletion.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
2024-11-04 08:39:54,588 ERROR __main__ Error generating answer: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
Traceback (most recent call last):
  File "/Users/param/Documents/PROJECTS/rag_chat/app.py", line 157, in main
    answer = get_answer(query, vector_store)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/src/rag_model.py", line 118, in get_answer
    completion = openai.ChatCompletion.create(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/chat_completion.py", line 25, in create
    return super().create(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_resources/abstract/engine_api_resource.py", line 153, in create
    response, _, api_key = requestor.request(
                           ^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 298, in request
    resp, got_stream = self._interpret_response(result, stream)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 700, in _interpret_response
    self._interpret_response_line(
  File "/Users/param/Documents/PROJECTS/rag_chat/myenv/lib/python3.12/site-packages/openai/api_requestor.py", line 765, in _interpret_response_line
    raise self.handle_error_response(
openai.error.InvalidRequestError: This model's maximum context length is 8192 tokens. However, your messages resulted in 9468 tokens. Please reduce the length of the messages.
2024-11-04 08:40:39,202 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-04 08:40:39,203 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-04 08:40:49,441 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-04 08:40:49,442 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-04 08:40:52,511 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-04 08:40:52,511 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-04 08:42:08,034 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-04 08:42:08,034 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-04 08:42:10,710 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-04 08:42:10,711 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-04 08:42:26,521 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-04 08:42:26,522 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-04 08:42:28,099 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-04 08:42:28,099 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-05 10:36:50,861 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-05 10:36:50,863 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-05 10:37:02,331 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-05 10:37:02,331 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-05 15:56:41,709 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-05 15:56:41,710 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-05 15:56:44,994 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-05 15:56:44,994 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-08 22:14:39,651 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:14:39,652 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-08 22:16:03,694 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:16:03,695 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-08 22:16:06,591 INFO absl Using default tokenizer.
2024-11-08 22:16:36,549 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:16:36,549 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-08 22:16:40,596 INFO absl Using default tokenizer.
2024-11-08 22:18:22,639 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:18:22,641 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-08 22:18:26,342 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:18:26,342 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-08 22:18:30,996 INFO absl Using default tokenizer.
2024-11-08 22:18:33,053 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:18:33,053 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-08 22:18:35,783 INFO absl Using default tokenizer.
2024-11-08 22:18:36,808 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:18:36,808 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-08 22:18:40,447 INFO absl Using default tokenizer.
2024-11-08 22:18:41,528 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:18:41,528 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-08 22:18:46,122 INFO absl Using default tokenizer.
2024-11-08 22:18:47,314 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:18:47,314 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/gtr-t5-large
2024-11-08 22:18:51,037 INFO absl Using default tokenizer.
2024-11-08 22:21:38,342 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:21:38,343 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-08 22:21:40,900 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:21:40,900 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-08 22:21:45,593 INFO absl Using default tokenizer.
2024-11-08 22:21:48,377 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:21:48,378 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-08 22:21:50,129 INFO absl Using default tokenizer.
2024-11-08 22:21:51,157 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:21:51,157 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-08 22:21:53,104 INFO absl Using default tokenizer.
2024-11-08 22:21:53,861 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:21:53,862 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-08 22:21:56,637 INFO absl Using default tokenizer.
2024-11-08 22:21:57,412 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 22:21:57,412 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-08 22:21:59,659 INFO absl Using default tokenizer.
2024-11-08 23:29:15,838 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 23:29:15,838 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-08 23:31:41,397 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 23:31:41,397 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
2024-11-08 23:33:03,682 INFO sentence_transformers.SentenceTransformer Use pytorch device_name: mps
2024-11-08 23:33:03,683 INFO sentence_transformers.SentenceTransformer Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2
